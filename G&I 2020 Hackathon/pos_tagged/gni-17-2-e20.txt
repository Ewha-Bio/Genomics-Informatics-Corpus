Improving/JJ
the/DT
CONTES/NN
method/NN
for/IN
normalizing/VBG
biomedical/JJ
text/NN
entities/VBZ
with/IN
concepts/NNS
from/IN
an/DT
ontology/NN
with/IN
(/(
almost/RB
)/)
no/DT
training/VBG
data/NNS
./.
====================
Entity/NN
normalization/NN
,/,
or/CC
entity/NN
linking/VBG
in/IN
the/DT
general/JJ
domain/NN
,/,
is/VBZ
an/DT
information/NN
extraction/NN
task/NN
that/DT
aims/VBZ
to/TO
annotate/bind/VB
multiple/JJ
words/expressions/NNS
in/IN
raw/JJ
text/RB
with/IN
semantic/JJ
references/VBZ
,/,
such/JJ
as/IN
concepts/NNS
of/IN
an/DT
ontology/NN
./.
====================
An/DT
ontology/JJ
consists/VBZ
minimally/RB
of/IN
a/DT
formally/RB
organized/VBN
vocabulary/NN
or/CC
hierarchy/NN
of/IN
terms/NNS
,/,
which/WDT
captures/NNS
knowledge/NN
of/IN
a/DT
domain/NN
./.
====================
Presently/RB
,/,
machine-learning/VBG
methods/NNS
,/,
often/RB
coupled/VBN
with/IN
distributional/JJ
representations/NNS
,/,
achieve/VBP
good/JJ
performance/NN
./.
====================
However/RB
,/,
these/DT
require/VB
large/JJ
training/VBG
datasets/NNS
,/,
which/WDT
are/VBP
not/RB
always/RB
available/JJ
,/,
especially/RB
for/IN
tasks/NNS
in/IN
specialized/VBN
domains/NNS
./.
====================
CONTES/NNS
(/(
CONcept-TErm/NN
System/NN
)/)
is/VBZ
a/DT
supervised/VBN
method/NN
that/DT
addresses/VBZ
entity/NN
normalization/NN
with/IN
ontology/NN
concepts/NNS
using/VBG
small/JJ
training/VBG
datasets/NNS
./.
====================
CONTES/NNS
has/VBZ
some/DT
limitations/NNS
,/,
such/JJ
as/IN
it/PRP
does/VBZ
not/RB
scale/VB
well/RB
with/IN
very/RB
large/JJ
ontologies/NNS
,/,
it/PRP
tends/VBZ
to/TO
overgeneralize/VB
predictions/NNS
,/,
and/CC
it/PRP
lacks/VBZ
valid/NN
representations/NNS
for/IN
the/DT
out-of-vocabulary/JJ
words/NNS
./.
====================
Here/RB
,/,
we/PRP
propose/VBP
to/TO
assess/VB
different/JJ
methods/NNS
to/TO
reduce/VB
the/DT
dimensionality/NN
in/IN
the/DT
representation/NN
of/IN
the/DT
ontology/NN
./.
====================
We/PRP
also/RB
propose/VBP
to/TO
calibrate/JJ
parameters/NNS
in/IN
order/NN
to/TO
make/VB
the/DT
predictions/NNS
more/RBR
accurate/JJ
,/,
and/CC
to/TO
address/VB
the/DT
problem/NN
of/IN
out-of-vocabulary/JJ
words/NNS
,/,
with/IN
a/DT
specific/JJ
method/NN
./.
====================
Availability/NN
:/:
The/DT
tool/NN
is/VBZ
on/IN
GitHub/NN
,/,
https/VBZ
:/:
//github.com/ArnaudFerre/CONTES/JJ
;/:
and/CC
the/DT
corpus/NN
and/CC
the/DT
ontology/NN
used/VBN
are/VBP
available/JJ
at/IN
BioNLP/NN
Shared-Task/NN
2016/CD
,/,
https/VBZ
:/:
//sites.google.com/site/bionlpst2016/tasks/bb2/NN
;/:
pre-calculated/VBN
embeddings/NNS
are/VBP
available/JJ
on/IN
demand/CC
./.
====================
The/DT
world/NN
of/IN
science/NN
produces/VBZ
a/DT
wealth/JJ
of/IN
knowledge/NN
in/IN
the/DT
form/NN
of/IN
scientific/JJ
literature/NN
,/,
a/DT
volume/NN
is/VBZ
so/RB
large/JJ
that/DT
it/PRP
is/VBZ
difficult/JJ
or/CC
even/RB
impossible/JJ
for/IN
scientists/NNS
to/TO
keep/VB
abreast/RB
of/IN
new/JJ
developments/NNS
[/(
1/CD
]/)
./.
====================
One/CD
solution/NN
to/TO
deal/JJ
with/IN
this/DT
problem/NN
is/VBZ
a/DT
natural/JJ
language/NN
processing/NN
approach/NN
known/VBN
as/IN
information/NN
extraction/NN
[/(
2/CD
]/)
./.
====================
Information/NN
extraction/NN
is/VBZ
the/DT
process/NN
of/IN
finding/NN
statements/NNS
on/IN
a/DT
specific/JJ
subject/JJ
in/IN
texts/NNS
[/(
3/CD
]/)
;/:
for/IN
example/NN
,/,
finding/NN
statements/NNS
about/RB
a/DT
protein/NN
binding/NN
to/TO
another/DT
protein/NN
,/,
or/CC
medication/NN
to/TO
treat/VB
a/DT
disease/NN
./.
====================
Normalization/NN
is/VBZ
the/DT
grounding/VBG
of/IN
an/DT
entity/NN
mentioned/VBN
in/IN
the/DT
text/NN
to/TO
an/DT
identifier/NN
in/IN
a/DT
data-/NN
or/CC
knowledge-base/JJ
[/(
4/CD
]/)
./.
====================
This/DT
is/VBZ
a/DT
core/NN
task/NN
of/IN
information/NN
extraction/NN
,/,
which/WDT
can/MD
be/VB
understood/VBN
as/IN
a/DT
classification/NN
problem/NN
:/:
classify/VB
entity/NN
mentions/NNS
into/IN
correct/JJ
class/NN
(/(
es/NNS
)/)
from/IN
a/DT
reference/NN
vocabulary/JJ
./.
====================
Current/JJ
normalization/NN
methods/NNS
try/NN
to/TO
address/VB
two/CD
main/JJ
problems/NNS
:/:
(/(
1/CD
)/)
the/DT
weak/JJ
generalizability/NN
to/TO
different/JJ
domains/tasks/NNS
and/CC
(/(
2/CD
)/)
the/DT
high/JJ
morphological/JJ
variability/NN
in/IN
entity/NN
mentions/NNS
and/CC
labels/NNS
of/IN
classes/NNS
./.
====================
Supervised/JJ
approaches/NNS
,/,
combined/JJ
with/IN
distributional/JJ
semantics/NNS
,/,
can/MD
bring/VB
some/DT
answers/NNS
./.
====================
In/IN
the/DT
general/JJ
domain/NN
,/,
where/WRB
the/DT
amount/NN
of/IN
training/VBG
data/NNS
is/VBZ
high/JJ
,/,
such/JJ
methods/NNS
already/RB
yield/VB
good/JJ
results/NNS
[/(
5/CD
]/)
./.
====================
But/CC
in/IN
specialized/VBN
domains/NNS
,/,
such/JJ
as/IN
biomedicine/JJ
,/,
where/WRB
there/EX
is/VBZ
often/RB
a/DT
lack/NN
of/IN
training/VBG
data/NNS
and/or/CC
a/DT
high/JJ
number/NN
of/IN
classes/NNS
,/,
these/DT
state/NN
of/IN
the/DT
art/NN
methods/NNS
seem/VBP
to/TO
have/VB
difficulties/VBZ
to/TO
keep/VB
a/DT
high/JJ
performance/NN
[/(
6/CD
]/)
./.
====================
Recently/RB
,/,
the/DT
CONTES/NN
(/(
CONcept-TErm/NN
System/NN
)/)
method/NN
[/(
7/CD
]/)
,/,
a/DT
supervised/VBN
method/NN
,/,
tries/VBZ
to/TO
address/VB
this/DT
problem/NN
by/IN
integrating/VBG
more/RBR
knowledge/NN
from/IN
domain/NN
ontologies/NNS
./.
====================
Ontologies/NNS
being/VBG
sometimes/RB
used/VBN
as/IN
reference/RB
for/IN
normalization/NN
tasks/NNS
,/,
their/PRP$
hypothesis/NN
is/VBZ
that/DT
deeper/IN
integration/NN
could/MD
improve/VB
the/DT
quality/NN
of/IN
the/DT
predictions/NNS
./.
====================
The/DT
method/NN
outperforms/NNS
existing/VBG
approaches/NNS
on/IN
the/DT
Bacteria/NN
Biotope/NN
(/(
BB/NN
)/)
normalization/NN
task/NN
of/IN
the/DT
BioNLP/NN
Shared/JJ
Task/NN
2016/CD
[/(
8/CD
]/)
./.
====================
The/DT
CONTES/NN
method/NN
,/,
however/RB
,/,
has/VBZ
two/CD
main/JJ
limitations/NNS
./.
====================
The/DT
first/JJ
is/VBZ
the/DT
size/NN
of/IN
the/DT
target/NN
ontology/NN
,/,
i.e./FW
,/,
large/JJ
sets/NNS
of/IN
concepts/NNS
prove/VB
difficult/JJ
to/TO
handle/VB
because/IN
of/IN
their/PRP$
high/JJ
dimensionality/NN
./.
====================
The/DT
second/JJ
is/VBZ
overgeneralization/JJ
:/:
CONTES/NNS
tends/NNS
to/TO
“/CD
play/VBP
it/PRP
safe/JJ
,/,
”/CD
and/CC
predict/VBP
more/RBR
general/JJ
concepts/NNS
./.
====================
Another/DT
possible/JJ
limitation/NN
comes/VBZ
from/IN
the/DT
impossibility/NN
of/IN
the/DT
method/NN
to/TO
represent/VB
words/NNS
unseen/JJ
during/IN
the/DT
training/VBG
step/NN
;/:
that/DT
is/VBZ
,/,
an/DT
out-of-vocabulary/JJ
word/NN
problem/NN
./.
====================
Here/RB
,/,
we/PRP
propose/VBP
to/TO
experiment/VB
different/JJ
methods/NNS
to/TO
reduce/VB
the/DT
dimensionality/NN
of/IN
the/DT
representation/NN
of/IN
the/DT
ontology/NN
structure/NN
,/,
and/CC
to/TO
calibrate/JJ
parameters/NNS
to/TO
make/VB
less/RBR
general/JJ
predictions/NNS
./.
====================
We/PRP
also/RB
propose/VBP
to/TO
integrate/VB
another/DT
method/NN
to/TO
produce/VB
word/NN
embeddings/NNS
,/,
which/WDT
addresses/VBZ
the/DT
problem/NN
of/IN
out-of-vocabulary/JJ
words/NNS
./.
====================
CONTES/NNS
[/(
7/CD
]/)
implements/VBZ
a/DT
supervised/VBN
machine/NN
learning/VBG
approach/NN
which/WDT
aims/VBZ
to/TO
improve/VB
the/DT
generalization/NN
limitations/NNS
of/IN
previous/JJ
machine/NN
learning/VBG
methods/NNS
,/,
especially/RB
when/WRB
few/JJ
training/VBG
datasets/NNS
are/VBP
available/JJ
,/,
with/IN
respect/NN
to/TO
the/DT
number/NN
of/IN
classes/NNS
./.
====================
CONTES/NNS
is/VBZ
an/DT
open-source/NN
tool/NN
written/CD
in/IN
Python/NN
(/(
https/NNS
:/:
//github.com/ArnaudFerre/CONTES/NN
)/)
./.
====================
The/DT
core/NN
of/IN
the/DT
method/NN
relies/VBZ
on/IN
two/CD
different/JJ
semantic/JJ
vector/NN
spaces/VBZ
:/:
(/(
1/CD
)/)
semantic/JJ
vector/NN
representations/NNS
of/IN
expressions/NNS
from/IN
a/DT
corpus/JJ
(/(
SSC/NN
for/IN
Semantic/JJ
Space/NN
of/IN
Corpus/NN
)/)
,/,
(/(
2/CD
)/)
semantic/JJ
vector/NN
representations/NNS
of/IN
concepts/NNS
from/IN
an/DT
ontology/NN
(/(
SSO/NN
for/IN
Semantic/JJ
Space/NN
of/IN
Ontology/NN
)/)
./.
====================
CONTES/NNS
can/MD
use/VB
all/DT
kinds/NNS
of/IN
semantic/JJ
vector/NN
representations/NNS
for/IN
SSC/NN
computations/NNS
,/,
such/JJ
as/IN
distributional/JJ
representations/NNS
./.
====================
It/PRP
particularly/RB
uses/VBZ
Word2Vec/NNP
word/NN
embeddings/NNS
[/(
9/CD
]/)
./.
====================
For/IN
SSO/NN
computation/NN
,/,
it/PRP
automatically/RB
builds/NNS
concept/NN
vectors/NNS
by/IN
analyzing/VBG
subsumption/NN
relationships/NNS
between/IN
all/DT
the/DT
concepts/NNS
of/IN
an/DT
ontology/NN
./.
====================
From/IN
the/DT
vector/NN
representations/NNS
,/,
it/PRP
learns/VBZ
a/DT
projection/NN
of/IN
the/DT
SSC/NN
into/IN
the/DT
SSO/NN
by/IN
globally/RB
maximizing/VBG
the/DT
cosine/NN
similarity/NN
between/IN
the/DT
expression/NN
vectors/NNS
and/CC
the/DT
associated/VBN
concept/NN
vectors/NNS
,/,
in/IN
the/DT
training/VBG
set/NN
./.
====================
In/IN
an/DT
unsupervised/JJ
setting/VBG
,/,
instead/RB
of/IN
using/VBG
(/(
expression/NN
,/,
concept/NN
)/)
associations/NNS
,/,
obtained/VBN
from/IN
an/DT
annotated/JJ
corpus/NN
for/IN
training/VBG
,/,
CONTES/NNS
can/MD
use/VB
(/(
label/NN
of/IN
concept/NN
,/,
concept/NN
)/)
intrinsic/JJ
associations/NNS
./.
====================
CONTES/NNS
showed/VBD
great/JJ
performance/NN
on/IN
the/DT
BB/NN
normalization/NN
task/NN
of/IN
BioNLP/NN
Shared/JJ
Task/NN
2016/CD
[/(
7/CD
]/)
./.
====================
Analysis/NN
of/IN
the/DT
results/NNS
suggested/VBD
that/IN
even/RB
mispredicted/JJ
concepts/NNS
were/VBD
not/RB
far/RB
from/IN
the/DT
ontology/NN
of/IN
the/DT
expected/VBN
concepts/NNS
./.
====================
Dimension/NN
reduction/NN
and/CC
concept/NN
embeddings/NNS
====================
The/DT
CONTES/NN
method/NN
has/VBZ
been/VBN
evaluated/VBN
as/IN
a/DT
normalization/NN
task/NN
of/IN
BB/NN
from/IN
the/DT
BioNLP/NN
Shared/JJ
Task/NN
of/IN
2016/CD
(/(
http/NN
:/:
//2016.bionlp-st.org/tasks/bb2/NN
)/)
./.
====================
This/DT
task/NN
consists/VBZ
of/IN
normalizing/VBG
entity/NN
mentions/NNS
,/,
in/IN
the/DT
raw/NN
text/NN
,/,
with/IN
concepts/NNS
of/IN
the/DT
OntoBiotope/NN
ontology/NN
./.
====================
OntoBiotope/RB
is/VBZ
an/DT
ontology/JJ
of/IN
microorganism/NN
habitats/NNS
,/,
and/CC
captures/NNS
classifications/NNS
used/VBN
by/IN
biologists/NNS
to/TO
describe/VB
microorganism/NN
isolation/NN
sites/NNS
(/(
e.g./FW
,/,
GenBank/NN
,/,
GOLD/NN
,/,
ATCC/NN
)/)
,/,
and/CC
contains/VBZ
around/IN
2,000/CD
concepts/NNS
,/,
which/WDT
is/VBZ
relatively/RB
small/JJ
,/,
compared/VBN
to/TO
other/JJ
biomedical/JJ
ontologies/NNS
,/,
but/CC
relatively/RB
high/JJ
,/,
for/IN
a/DT
machine-learning/JJ
classification/NN
problem/NN
./.
====================
In/IN
comparison/NN
,/,
the/DT
Gene/NN
Ontology/NN
contains/VBZ
around/IN
50,000/CD
concepts/NNS
and/CC
the/DT
Unified/JJ
Medical/JJ
Language/JJ
System/NN
meta-thesaurus/NN
contains/VBZ
more/RBR
than/IN
one/CD
million/NN
concepts/NNS
./.
====================
CONTES/NNS
does/VBZ
not/RB
currently/RB
address/VB
normalization/JJ
with/IN
these/DT
large/JJ
ontologies/NNS
,/,
due/JJ
to/TO
its/PRP$
high/JJ
dimension/NN
,/,
but/CC
rather/RB
implies/VBZ
the/DT
representation/NN
of/IN
the/DT
ontology/NN
concepts/NNS
./.
====================
Overgeneralization/NN
====================
CONTES/NNS
takes/VBZ
advantage/NN
of/IN
the/DT
ontology/NN
structure/NN
given/VBN
by/IN
a/DT
set/NN
of/IN
Is-A/NN
relations/NNS
between/IN
concepts/NNS
./.
====================
The/DT
representation/NN
of/IN
the/DT
ontology/NN
structure/NN
into/IN
SSO/NN
vectors/NNS
allows/VBZ
CONTES/NNS
to/TO
generalize/VB
training/VBG
examples/NNS
,/,
and/CC
classify/VB
terms/NNS
never/RB
seen/VBN
before/IN
with/IN
more/RBR
general/JJ
concepts/NNS
./.
====================
However/RB
,/,
error/NN
analysis/NN
on/IN
the/DT
BB/NN
development/NN
set/NN
showed/VBD
that/IN
CONTES/NNS
tends/NNS
to/TO
overgeneralize/VB
its/PRP$
predictions/NNS
,/,
and/CC
often/RB
predicts/VBZ
too/RB
general/JJ
concepts/NNS
./.
====================
Out-of-vocabulary/JJ
words/NNS
====================
CONTES/NNS
uses/VBZ
the/DT
Word2Vec-CBOW/NN
and/CC
Word2Vec-SkipGram/NN
methods/NNS
to/TO
produce/VB
word/NN
embeddings/NNS
,/,
and/CC
these/DT
methods/NNS
do/VBP
not/RB
compute/VB
valid/NN
representations/NNS
for/IN
out-of-vocabulary/JJ
words/NNS
./.
====================
Thus/RB
,/,
CONTES/NN
does/VBZ
not/RB
yet/RB
address/VB
the/DT
problem/NN
of/IN
the/DT
out-of-vocabulary/JJ
words/NNS
./.
====================
The/DT
lack/NN
of/IN
valid/JJ
representations/NNS
for/IN
some/DT
words/NNS
is/VBZ
likely/JJ
to/TO
bias/NNS
the/DT
performance/NN
of/IN
the/DT
method/NN
./.
====================
For/IN
example/NN
,/,
in/IN
French/JJ
or/CC
Spanish/JJ
,/,
most/JJS
verbs/RB
have/VBP
more/RBR
than/IN
forty/NN
different/JJ
inflected/JJ
forms/NNS
,/,
while/IN
the/DT
Finnish/JJ
language/NN
has/VBZ
fifteen/CD
cases/NNS
for/IN
nouns/NNS
./.
====================
These/DT
languages/NNS
contain/VBP
many/JJ
word/NN
forms/NNS
that/DT
occur/VBP
rarely/RB
(/(
or/CC
not/RB
at/IN
all/DT
)/)
in/IN
the/DT
training/VBG
corpus/NN
,/,
making/VBG
it/PRP
difficult/JJ
to/TO
learn/VB
good/JJ
word/NN
representations/NNS
./.
====================
Materials/NNS
====================
For/IN
our/PRP$
experiments/NNS
,/,
we/PRP
use/VBP
the/DT
current/JJ
source/NN
code/VBP
of/IN
CONTES/NN
(/(
https/NNS
:/:
//github.com/ArnaudFerre/CONTES/JJ
)/)
,/,
and/CC
datasets/NNS
and/CC
the/DT
OntoBiotope/NN
ontology/RB
from/IN
the/DT
BB/NN
normalization/NN
task/NN
of/IN
the/DT
BioNLP/NN
Shared/JJ
Task/NN
2016/CD
./.
====================
We/PRP
also/RB
downscaled/VBD
word/NN
embeddings/NNS
computed/VBN
from/IN
a/DT
reduced/VBN
corpus/NN
(/(
40,000/CD
words/NNS
)/)
,/,
in/IN
order/NN
to/TO
speed/JJ
up/IN
the/DT
experimental/JJ
cycle/NN
./.
====================
Dimensionality-reduction/NN
techniques/NNS
====================
We/PRP
aimed/VBD
to/TO
reduce/VB
the/DT
dimension/NN
of/IN
the/DT
SSO/NN
to/TO
make/VB
CONTES/NNS
scalable/JJ
to/TO
larger/JJR
ontologies/NNS
./.
====================
We/PRP
have/VBP
used/VBN
three/CD
well-known/JJ
techniques/NNS
to/TO
reduce/VB
the/DT
dimension/NN
of/IN
the/DT
semantic/JJ
space/NN
of/IN
the/DT
ontology/NN
:/:
principal/JJ
component/NN
analysis/NN
(/(
PCA/NN
)/)
,/,
multidimensional/JJ
scaling/VBG
(/(
MDS/NN
)/)
,/,
and/CC
t-distributed/JJ
Stochastic/JJ
Neighbor/CC
Embedding/JJ
(/(
t-SNE/NN
)/)
./.
====================
PCA/NN
and/CC
MDS/NN
have/VBP
already/RB
been/VBN
tested/VBN
,/,
with/IN
similar/JJ
results/NNS
[/(
7/CD
]/)
./.
====================
Here/RB
,/,
t-SNE/NN
is/VBZ
a/DT
nonlinear/JJ
method/NN
to/TO
reduce/VB
dimensions/NNS
,/,
with/IN
the/DT
aim/NN
to/TO
retain/VB
the/DT
similarity/NN
of/IN
vectors/NNS
between/IN
the/DT
initial/JJ
space/NN
and/CC
the/DT
embedded/JJ
space/NN
./.
====================
All/DT
these/DT
methods/NNS
can/MD
be/VB
used/VBN
with/IN
the/DT
machine/NN
learning/VBG
Python/NN
library/NN
scikit-learn/NN
(/(
https/NNS
:/:
//scikit-learn.org/stable//JJ
)/)
./.
====================
Concept/IN
embeddings/NNS
====================
Another/DT
solution/NN
to/TO
overcome/VB
the/DT
problem/NN
of/IN
high-dimensional/JJ
vectors/NNS
of/IN
ontology/JJ
concepts/NNS
,/,
in/IN
CONTES/NN
,/,
is/VBZ
to/TO
directly/RB
learn/VB
embeddings/NNS
for/IN
each/DT
concept/NN
./.
====================
With/IN
the/DT
success/NN
of/IN
the/DT
word-embedding/VBG
methods/NNS
,/,
we/PRP
considered/VBD
graph-based/VBN
embeddings/NNS
,/,
and/CC
used/VBN
Node2Vec/JJ
[/(
10/CD
]/)
to/TO
compute/VB
the/DT
concept/NN
vectors/NNS
(/(
SSO/NN
)/)
of/IN
ontologies/NNS
for/IN
CONTES/NN
./.
====================
Node2Vec/JJ
is/VBZ
a/DT
method/NN
to/TO
learn/VB
embeddings/NNS
for/IN
nodes/NNS
in/IN
networks/NNS
./.
====================
The/DT
approach/NN
relies/VBZ
on/IN
a/DT
maximization/NN
of/IN
the/DT
likelihood/NN
of/IN
preserving/VBG
neighborhoods/NNS
of/IN
nodes/NNS
in/IN
networks/NNS
./.
====================
It/PRP
considers/VBZ
the/DT
hypothesis/NN
that/IN
highly/RB
connected/VBN
nodes/NNS
of/IN
the/DT
same/JJ
community/NN
have/VBP
similar/JJ
embeddings/NNS
,/,
and/CC
that/IN
nodes/NNS
having/VBG
similar/JJ
structural/JJ
roles/NNS
in/IN
networks/NNS
,/,
have/VBP
similar/JJ
embeddings/NNS
[/(
10/CD
]/)
./.
====================
For/IN
CONTES/NN
,/,
Node2Vec/JJ
has/VBZ
the/DT
advantage/NN
of/IN
being/VBG
applicable/JJ
to/TO
ontologies/NNS
,/,
and/CC
it/PRP
also/RB
directly/RB
provides/VBZ
low-dimensional/JJ
vectors/NNS
./.
====================
Although/IN
nodes/NNS
in/IN
ontologies/NNS
are/VBP
not/RB
generally/RB
highly/RB
connected/VBN
,/,
as/IN
can/MD
be/VB
the/DT
social/JJ
network/NN
graphs/NNS
for/IN
example/NN
,/,
we/PRP
expect/VBP
that/DT
Node2Vec/JJ
can/MD
also/RB
be/VB
suitable/JJ
for/IN
ontologies/NNS
by/IN
providing/VBG
low-dimensional/JJ
vectors/NNS
for/IN
ontology/JJ
concepts/NNS
,/,
based/VBN
on/IN
the/DT
neighborhoods/NNS
expressed/VBN
in/IN
relationships/NNS
such/JJ
as/IN
“/RB
Is-A/NN
”/CD
and/CC
“/FW
sameAs/NNS
./.
”/CD
====================
The/DT
experiments/NNS
with/IN
Node2vec/JJ
allowed/VBD
us/PRP
to/TO
obtain/VB
compact/VBP
vector/NN
representations/NNS
for/IN
the/DT
Ontobiotope/NN
concepts/NNS
,/,
which/WDT
opens/VBZ
the/DT
possibility/NN
for/IN
larger/JJR
ontologies/NNS
./.
====================
Overgeneralization/NN
====================
Our/PRP$
hypothesis/NN
was/VBD
that/DT
CONTES/NNS
overgeneralization/NN
is/VBZ
caused/VBN
by/IN
giving/VBG
too/RB
much/RB
weight/NN
to/TO
the/DT
individual/JJ
concept/NN
ancestors/NNS
,/,
in/IN
the/DT
vectors/NNS
representing/VBG
the/DT
ontology/NN
structure/NN
./.
====================
CONTES/NNS
uses/VBZ
the/DT
“/NN
Ancestry/NN
”/CD
method/NN
where/WRB
each/DT
concept/NN
is/VBZ
represented/VBN
as/IN
a/DT
vector/NN
having/VBG
a/DT
number/NN
of/IN
dimensions/NNS
equal/JJ
to/TO
the/DT
total/JJ
number/NN
of/IN
concepts/NNS
in/IN
the/DT
ontology/NN
./.
====================
For/IN
a/DT
given/VBN
concept/NN
,/,
the/DT
dimension/NN
corresponding/VBG
to/TO
the/DT
concept/NN
itself/PRP
or/CC
to/TO
one/CD
of/IN
its/PRP$
ancestors/NNS
is/VBZ
set/NN
to/TO
one/CD
(/(
1/CD
)/)
./.
====================
All/DT
other/JJ
dimensions/NNS
are/VBP
set/NN
to/TO
zero/CD
(/(
0/CD
)/)
./.
====================
The/DT
representation/NN
has/VBZ
the/DT
advantage/NN
of/IN
preserving/VBG
the/DT
required/VBN
information/NN
to/TO
rebuild/JJ
the/DT
ontology/NN
structure/NN
./.
====================
However/RB
,/,
all/DT
ancestors/NNS
have/VBP
the/DT
same/JJ
weight/NN
regardless/RB
of/IN
the/DT
number/NN
of/IN
intermediate/JJ
concepts/NNS
./.
====================
We/PRP
experimented/VBD
with/IN
a/DT
method/NN
where/WRB
the/DT
dimension/NN
value/NN
is/VBZ
decreased/VBN
using/VBG
a/DT
function/NN
that/IN
depends/VBZ
on/IN
(/(
1/CD
)/)
the/DT
depth/NN
between/IN
the/DT
concept/NN
and/CC
the/DT
ancestor/NN
,/,
and/CC
(/(
2/CD
)/)
a/DT
constant/JJ
parameter/NN
used/VBN
to/TO
set/VB
a/DT
“/JJ
decay/NN
”/CD
factor/NN
./.
====================
The/DT
function/NN
is/VBZ
wd/VBN
,/,
where/WRB
w/JJ
is/VBZ
the/DT
decay/NN
factor/NN
whose/WP$
range/NN
is/VBZ
[/(
0,1/CD
]/)
,/,
and/CC
d/FW
is/VBZ
the/DT
number/NN
of/IN
intermediate/JJ
concepts/NNS
between/IN
the/DT
target/NN
concept/IN
and/CC
its/PRP$
ancestor/NN
(/(
zero/CD
for/IN
itself/PRP
,/,
one/CD
for/IN
its/PRP$
direct/JJ
parent/JJ
)/)
./.
====================
Out-of-vocabulary/JJ
words/NNS
====================
New/NN
words/NNS
currently/RB
required/VBN
to/TO
recalculate/VB
the/DT
word/NN
embeddings/NNS
in/IN
CONTES/NN
./.
====================
Methods/NNS
like/IN
FastText/JJ
are/VBP
designed/VBN
to/TO
obtain/VB
embeddings/NNS
for/IN
words/NNS
even/RB
if/IN
they/PRP
are/VBP
unknown/JJ
from/IN
the/DT
training/VBG
data/NNS
./.
====================
For/IN
our/PRP$
experiments/NNS
,/,
we/PRP
considered/VBD
FastText/NN
[/(
11/CD
]/)
as/IN
a/DT
solution/NN
to/TO
tackle/JJ
the/DT
problem/NN
of/IN
out-of-vocabulary/JJ
words/NNS
./.
====================
The/DT
FastText/JJ
method/NN
is/VBZ
based/VBN
on/IN
the/DT
skip-gram/NN
model/NN
[/(
9,11/CD
]/)
,/,
and/CC
takes/VBZ
into/IN
account/VBP
subword/NN
information/NN
to/TO
produce/VB
word/NN
embeddings/NNS
./.
====================
It/PRP
also/RB
considers/VBZ
each/DT
word/NN
as/IN
a/DT
bag/JJ
of/IN
character/NN
n-grams/NNS
and/CC
gives/VBZ
the/DT
word/NN
representation/NN
by/IN
summing/VBG
the/DT
representations/NNS
of/IN
the/DT
character/NN
n-grams/NNS
[/(
11/CD
]/)
./.
====================
To/TO
investigate/VB
the/DT
effects/NNS
of/IN
unknown/JJ
words/NNS
,/,
FastText/JJ
is/VBZ
added/VBN
to/TO
CONTES/NN
as/IN
another/DT
method/NN
,/,
and/CC
we/PRP
have/VBP
conducted/VBN
experiments/NNS
with/IN
a/DT
subset/NN
of/IN
the/DT
datasets/NNS
of/IN
the/DT
BB/NN
normalization/NN
task/NN
./.
====================
Dimension/NN
reduction/NN
====================
As/IN
shown/VBN
in/IN
[/(
7/CD
]/)
,/,
the/DT
PCA/NN
and/CC
MDS/NN
methods/NNS
had/VBD
similar/JJ
behavior/NN
,/,
and/CC
they/PRP
both/CC
reduced/VBD
the/DT
performance/NN
of/IN
the/DT
CONTES/NN
method/NN
(/(
Fig/NN
./.
====================
1/CD
)/)
./.
====================
The/DT
score/NN
is/VBZ
based/VBN
on/IN
the/DT
semantic/JJ
similarity/NN
proposed/VBN
by/IN
Wang/NN
et/FW
al/JJ
./.
====================
[/(
12/CD
]/)
./.
====================
The/DT
gap/NN
is/VBZ
small/JJ
when/WRB
the/DT
reduction/NN
is/VBZ
low/JJ
,/,
but/CC
increases/VBZ
beyond/IN
the/DT
acceptable/JJ
performance/NN
threshold/NN
when/WRB
it/PRP
reaches/VBZ
60/CD
%/NN
reduction/NN
./.
====================
The/DT
sklearn/NN
t-SNE/NN
method/NN
is/VBZ
not/RB
scalable/JJ
:/:
increasing/VBG
the/DT
reduced/VBN
space/NN
dimension/NN
number/NN
seems/VBZ
to/TO
prevent/VB
any/DT
execution/NN
of/IN
reduction/NN
./.
====================
The/DT
sklearn/NN
description/NN
of/IN
the/DT
method/NN
mentions/NNS
that/DT
“/RB
It/PRP
is/VBZ
highly/RB
recommended/VBN
to/TO
use/VB
another/DT
dimensionality/NN
reduction/NN
method/NN
[/(
.../:
]/)
to/TO
reduce/VB
the/DT
number/NN
of/IN
dimensions/NNS
to/TO
a/DT
reasonable/JJ
amount/NN
(/(
e.g./FW
,/,
50/CD
)/)
,/,
if/IN
the/DT
number/NN
of/IN
features/NNS
is/VBZ
very/RB
high/JJ
”/NN
(/(
https/NNS
:/:
//scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html/JJ
)/)
./.
====================
The/DT
initial/JJ
goal/NN
of/IN
the/DT
method/NN
is/VBZ
indeed/RB
to/TO
enable/VB
a/DT
2D/3D/JJ
representation/NN
of/IN
a/DT
high-dimensional/JJ
space/NN
./.
====================
Overgeneralization/NN
====================
We/PRP
experimented/VBD
the/DT
aforementioned/VBN
function/NN
for/IN
computing/VBG
SSO/NN
by/IN
varying/VBG
the/DT
decay/NN
factor/NN
from/IN
zero/CD
to/TO
one/CD
,/,
by/IN
steps/NNS
of/IN
0.1/CD
./.
====================
With/IN
a/DT
decay/NN
factor/NN
of/IN
one/CD
,/,
the/DT
function/NN
produces/VBZ
the/DT
same/JJ
vectors/NNS
as/IN
the/DT
original/JJ
“/NN
Ancestry/NN
”/CD
method/NN
./.
====================
With/IN
a/DT
decay/NN
factor/NN
of/IN
zero/CD
,/,
the/DT
function/NN
reproduces/VBZ
a/DT
“/JJ
One/CD
Hot/NN
”/CD
representation/NN
./.
====================
We/PRP
measured/VBD
the/DT
impact/NN
of/IN
the/DT
decay/NN
factor/NN
on/IN
the/DT
BBs/NNS
development/NN
set/NN
,/,
using/VBG
two/CD
metrics/NNS
./.
====================
The/DT
first/JJ
metric/JJ
is/VBZ
the/DT
official/JJ
BBs/NNS
precision/NN
measure/VB
that/DT
is/VBZ
a/DT
mean/JJ
of/IN
the/DT
semantic/JJ
similarity/NN
between/IN
predicted/VBN
and/CC
reference/VBP
normalizations/NNS
,/,
and/CC
the/DT
semantic/JJ
similarity/NN
rewards/VBZ
predictions/NNS
that/DT
are/VBP
near/JJ
the/DT
reference/NN
./.
====================
The/DT
second/JJ
metric/JJ
is/VBZ
a/DT
strict/JJ
measure/VB
,/,
where/WRB
the/DT
prediction/NN
is/VBZ
only/RB
rewarded/VBD
if/IN
it/PRP
is/VBZ
exactly/RB
the/DT
same/JJ
as/IN
the/DT
reference/NN
./.
====================
The/DT
results/NNS
are/VBP
shown/VBN
in/IN
Fig/NN
./.
====================
2/CD
./.
====================
The/DT
strict/JJ
measure/VB
is/VBZ
necessarily/RB
lower/JJR
,/,
since/IN
it/PRP
does/VBZ
not/RB
reward/VB
generalization/NN
or/CC
near-misses/NNS
./.
====================
With/IN
the/DT
semantic/JJ
similarity/NN
measure/VB
,/,
we/PRP
reproduced/VBD
the/DT
results/NNS
published/VBN
in/IN
Ferre/JJ
et/FW
al./FW
’/CD
s/NNS
study/NN
[/(
13/CD
]/)
where/WRB
“/CD
Ancestry/NN
”/CD
(/(
decay/NN
=/JJ
1/CD
)/)
outperforms/VBZ
“/CD
One/CD
Hot/NN
”/CD
(/(
decay/NN
=/JJ
0/CD
)/)
./.
====================
We/PRP
also/RB
demonstrated/VBD
an/DT
optimal/JJ
decay/NN
factor/NN
between/IN
0.6/CD
and/CC
0.7/CD
that/DT
further/JJ
improves/VBZ
CONTES/NNS
performance/NN
./.
====================
With/IN
the/DT
strict/JJ
measure/VB
,/,
we/PRP
notice/RB
that/DT
“/CD
One/CD
Hot/NN
”/CD
is/VBZ
the/DT
best/JJS
representation/NN
./.
====================
Out-of-vocabulary/JJ
words/NNS
====================
Concerning/VBG
out-of-vocabulary/JJ
words/NNS
,/,
our/PRP$
experiments/NNS
showed/VBD
no/DT
significant/JJ
difference/NN
in/IN
performance/NN
between/IN
FastText/NN
and/CC
Word2vec/NNP
./.
====================
Perspectives/NNS
and/CC
conclusion/NN
====================
In/IN
this/DT
work/NN
,/,
we/PRP
addressed/VBD
some/DT
important/JJ
issues/NNS
to/TO
normalize/VB
biomedical/JJ
text/NN
entities/VBZ
using/VBG
concepts/NNS
of/IN
ontologies/NNS
,/,
in/IN
the/DT
context/NN
of/IN
small/JJ
training/VBG
data/NNS
./.
====================
Reduction/NN
of/IN
dimensions/NNS
,/,
in/IN
the/DT
representation/NN
of/IN
the/DT
concepts/NNS
,/,
appears/VBZ
to/TO
be/VB
a/DT
central/JJ
issue/NN
when/WRB
trying/VBG
to/TO
support/VB
large/JJ
ontologies/NNS
./.
====================
We/PRP
also/RB
tested/VBN
PCA/NN
and/CC
MDS/NN
methods/NNS
,/,
and/CC
showed/VBD
that/IN
the/DT
gap/NN
in/IN
performance/NN
between/IN
these/DT
methods/NNS
,/,
and/CC
the/DT
original/JJ
CONTES/NN
,/,
rapidly/RB
widens/NNS
when/WRB
the/DT
reduction/NN
gets/NNS
high/JJ
./.
====================
We/PRP
also/RB
introduced/VBD
concept/NN
embeddings/NNS
to/TO
tackle/JJ
the/DT
dimensional/JJ
reduction/NN
,/,
and/CC
we/PRP
plan/VBP
to/TO
perform/VB
follow-up/JJ
experiments/NNS
with/IN
Node2Vec/JJ
by/IN
more/RBR
generally/RB
considering/VBG
the/DT
other/JJ
relationships/NNS
between/IN
the/DT
concepts/NNS
in/IN
the/DT
ontologies/NNS
./.
====================
New/NN
and/CC
large/JJ
ontologies/NNS
will/MD
be/VB
addressed/VBN
in/IN
future/JJ
work/NN
./.
====================
By/IN
tackling/VBG
overgeneralization/NN
,/,
our/PRP$
results/NNS
showed/VBD
a/DT
way/NN
to/TO
enhance/VB
CONTES/NN
with/IN
a/DT
function/NN
weighing/VBG
the/DT
ontology/NN
concepts/NNS
,/,
based/VBN
on/IN
the/DT
depths/NNS
between/IN
the/DT
concepts/NNS
and/CC
their/PRP$
ancestors/NNS
./.
====================
Out-of-vocabulary/JJ
words/NNS
is/VBZ
addressed/VBN
with/IN
FastText/JJ
,/,
but/CC
it/PRP
does/VBZ
not/RB
show/VB
significant/JJ
differences/NNS
,/,
compared/VBN
to/TO
results/NNS
without/IN
FastText/NN
,/,
but/CC
we/PRP
expect/VBP
that/DT
results/VBZ
may/MD
change/VB
with/IN
larger/JJR
datasets/NNS
./.
====================
The/DT
experiments/NNS
were/VBD
conducted/VBN
under/IN
specific/JJ
conditions/NNS
with/IN
reduced/VBN
datasets/NNS
,/,
and/CC
future/JJ
work/NN
will/MD
consider/VB
that/DT
point/VBP
./.
====================
Performance/NN
of/IN
the/DT
method/NN
,/,
according/VBG
to/TO
the/DT
dimension/NN
reduction/NN
./.
====================
Results/NNS
with/IN
semantic/JJ
similarity/NN
and/CC
strict/JJ
measures/NNS
./.
====================
