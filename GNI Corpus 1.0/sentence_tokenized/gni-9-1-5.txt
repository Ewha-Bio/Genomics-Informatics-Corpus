Recent Progresses in the Linguistic Modeling of Biological Sequences Based on Formal Language Theory.

Treating genomes just as languages raises the possibility of producing concise generalizations about information in biological sequences.

Grammars used in this way would constitute a model of underlying biological processes or structures, and that grammars may, in fact, serve as an appropriate tool for theory formation.

The increasing number of biological sequences that have been yielded further highlights a growing need for developing grammatical systems in bioinformatics.

The intent of this review is therefore to list some bibliographic references regarding the recent progresses in the field of grammatical modeling of biological sequences.

This review will also contain some sections to briefly introduce basic knowledge about formal language theory, such as the Chomsky hierarchy, for non-experts in computational linguistics, and to provide some helpful pointers to start a deeper investigation into this field.

Keywords: natural language processing, Chomsky hierarchy, bioinformatics, formal language theoryIn formal language theory, a language is simply a set of strings of characters drawn from some alphabet, where the alphabet is a set of symbols.

The challenge of computational linguistics is to find concise ways of specifying a given language L, preferably in a way that reflects some underlying model of the source of that language (Searls, 1993).

For example, we can use informal descriptions of the language LGENE that make use of a natural description, as follows.

However, simply exhaustively enumerating languages as below is imopssible:  LGENE = { atg, atga, atgt, atgg, atgc, atgaa, atgtt, atggg, atgcc, …}  Regular expression, a widelyused method for specifying simple languages, can be used to define LGENE, in a more concise way, as atg(a|t|g|c)*.

Alternatively, the same language can be defined in formal grammar rules (Hopcroft and Ullman, 1979), as in Fig.

1(a), and as a finite state automata as in Fig.

1(b).

Formal grammar (N, T, P, S) consists of: a finite set of terminal symbols (T: usually represented by lowercase letters), a finite set of nonterminal symbols (N: usually represented by uppercase letters), a finite set of production rules with a left and a righthand side consisting of a sequence of these symbols (P), and a start symbol (S).

Those readers requiring a more detailed introduction to formal language theory and bioinformatics are referred to chapter 2 of the book, “Artificial intelligence and molecular biology” (Searls, 2002; Searls, 2003).

A derivation is a rewriting of a string using the rules of the grammar.

Thus, a rule may be applied to a sequence of symbols by replacing an occurrence of the symbols on the lefthand side of the rule with those that appear on the righthand side.

For example, by applying the production rules in Fig.

1(a), the string “atgcca” can be derived from the nonterminal S, by applying a series of derivations: S → aA → atB → atgC → atgcC → atgccC → atgccaC → atgcca.

The simple grammar topologies or even less expressive formalisms can be sufficient to characterize biological sequences in many cases.

But they cannot model longterm dependencies such as contacts of amino acids that are far in the sequence but close in the physical folding of the protein.

In order to model higherorder structures of biological sequences, we need more powerful grammatical systems based on formal language theory, as a biological sequence can be thought of as a richlyexpressive language for specifying the structures and processes of life.

Searls (1988) initiated pioneering works to view biological sequences   simply as linguistic sentences (Searls, 1988).

When we view these sequences just as strings on alphabets, a grammatical representation based on formal language theory can be applied to various problems for biological sequence analyses.

Indeed, linguistic grammars have been used to model and predict multiple sequence alignments, transcription binding sites, RNA folding and secondary structures, integrons, insertion sequences, genes, and gene cassettes.

The remainder of this paper introduces the Chomsky hierarchy (Chomsky, 1957), and surveys bioinformatics approaches based on regular grammars, contextfree grammars, and context-sensitive grammars, to model various types of biological sequences.

In Transformational Analysis (Chomsky, 1955) and Syntactic Structures (Chomsky, 1957), Chomsky initiated the theory of generative grammar and of the theory of formal languages as a branch of mathematical logic.

The Chomsky hierarchy refers to a containment hierarchy of classes of formal grammars.

Fig.

2 summarizes each of four types of grammar.

In increasing complexity and power, they are called type-3, type-2, type-1, and type-0 - each one a subclass of the next.

Each type can be defined by a class of grammars, as indicated in Fig.

2.

Exactly how much linguistic power is actually required to model biological sequences is one of the ultimate questions in bioinformatics.

The following sections survey various approaches based on grammar formalisms, ordered by type3 (regular grammars), type-2 (context-free grammars), and type1 (context-sensitive grammars) languages.

Regular grammars, the first level of Chomsky's hierarchy, in Fig.

2 (Type3 grammars), generate regular languages.

Regular grammar is a formal grammar (N, T, P, S), such that all of the production rules in P are of one of the following forms:  1.

A → a  where A is a nonterminal in N and a is a terminal in T  2.

A → Ba (or A → aB)  where A and B are in N and a is in T  3.

A → ε  where A is in N and ε is the empty string.

Regular languages can be described by regular expressions, and they are commonly used to define search patterns and the lexical structure of languages.

Head (Head, 1987) initiated a formal analysis of the generative power of recombinatorial behaviors in biological sequences.

His persistent splicing languages are shown to coincide with a class of regular languages.

Brazma,   Jonassen, Eidhammer, and Gilbert (Brazma et al., 1998) surveyed approaches and algorithms used for the automatic discovery of patterns with expressive power in the class of regular languages.

An early work toward learning grammars based on regular expressions is the work of Yokomori (Yokomori, 1994) on learning a special type of regular language called a locally testable language from positive data, and its application in identifying the protein αchain region in amino acid sequences.

Peris’s group used a grammatical approach to predict coiledcoil proteins (Peris et al., 2006) and transmembrane domains in proteins (Peris et al., 2008).

Actually, if we extend our scope, and consider the fact that most of the works on patterns (Liew et al., 2005) can be represented by regular grammars, the fields of so-called motif bioinformatics belong to type3 grammar.

Also, many of the current motif databases are based on the expressive power of regular grammar.

To mention one among a few, the Prosite (Hulo et al., 2006) and ProRule (Sigrist et al., 2005) databases, one of the most successful databases, define signatures of known families of amino acid sequences that are expressed in sub-regular expressions.

On the other hand, a problem faced in these kinds of large-scale realistic grammars is that more than one production rule may apply to a structure.

Naturally, probabilistic grammars have often been used to circumvent these ambiguities by using a probabilistic model consisting of a non-probabilistic model plus some numerical quantities.

Among probabilistic grammars, the profile Hidden Markov Model, or pHMM (Eddy, 1998; Krogh et al., 1994) is most closely related to regular grammars, because an n-gram is a subsequence of n items from a given sequence, and language models built from n-grams are actually (n-1)-order Markov models.

Coste and Kerbellec.

(2005) showed a successful application of the classical state merging framework developed in grammatical inference, to learning automata on selection and ordering of similar fragments to be merged, and on physico-chemical property identification (Coste and Kerbellec., 2005).

Their work offers the opportunity to learn more expressive topologies than those of pHMMs, while still benefiting from the weighting schemes developed for pHMM.

Recently, Tsafnat et al.

(2011) used computational grammar inference methods to automate LGS (Larger than Gene Structures) discovery (Tsafnat et al., 2011).

The authors compared the ability of six algorithms to infer LGS grammars from DNA sequences annotated with genes and other short sequences.

As we have discussed, regular grammars (including motif bioinformatics) are the most prevalently used formalism in bioinformatics.

Still, the limitations of regular grammar are that regular grammar can only model the primary structures of biological sequences and cannot explicitly model higherorder structures such as secondary structures of RNAs and tertiary structures of proteins.

In order to model higherorder structures of biological sequences, many researchers have used more powerful grammatical systems.

As stated in the previous section, regular grammar has its limitations, and the approaches based on type3 language have been criticized, especially by Chomskyans, because they lack any explicit representation of longrange dependency.

In the Chomsky hierarchy in Fig.

2, type2 grammar, or contextfree grammar generates contextfree language, which is more powerful than regular grammar.

In terms of production rules, every production of a contextfree grammar is of the form:  A → w  where A is a single nonterminal symbol, w is a string, and the lefthand side of a production rule is always a single nonterminal symbol.

We can specify a simple grammar representing an RNA palindrome in the following way:  S → aSu   S → uSa   S → gSc   S → cSg  …  The grammar above captures longrange dependency.

For example, the palindrome string “aug…cau” can be derived from the nonterminal S, by applying a series of derivations: S → aSu → auSau → augScau …  Numerous attempts have been made to solve the problems of modeling of families of homologous RNA sequences, and predicting RNA secondary structure prediction techniques, since computational recognition based on type2 grammar has been shown to perform in polynomial time.

In the 1990’s, DCGs (definite clause grammars) (Pereira and Warren, 1980), a kind of logic programmingbased grammars, were adopted to study socalled “DNA linguistics” (Searls, 1993).

Later, DCGs and the Prolog programming language were used in modeling gene regulation (ColladoVides, 1992; Rosenblueth et al., 1996), benefiting from features such as parameterpassing and arbitrary Prolog code embeddings.

Basic Gene Grammar, an attempt to simplify the representations of DNA sequences, and with expressive power equivalent to that of DCG, has been used to model and predict   transcription binding sites (Leung et al., 2001).

Nevill-Manning and Whitten (NevilleManning and Whitten, 1997) initiated an attempt to produce context-free grammars of biological sequences, in an automatic way.

Later, similar attempts have been made by other researchers, along this line (Apostolico and Lonardi, 2000; Carrascosa et al., 2011; Cherniavsky and Ladner, 2004; Lanctot et al., 2000), generating grammars based on repeated phrases.

This task can be formalized as the problem of finding the smallest context-free grammar by recursively replacing the repeats by non-terminals.

Muggleton et al.

(2001) investigated whether Chomsky-like grammar representations are useful for learning cost-effective, comprehensible predictors of members of biological sequence families (Muggleton et al., 2001).

As a case study, they proved that the most costeffective, comprehensible multi-strategy predictor of human neuropeptide precursors employ context-free grammar.

Like the cases for type-3 grammars, many of the attempts based on type-2 grammars also used stochastic methods, especially to resolve difficulties that arise because longer sentences are highly ambiguous when processed with realistic grammars.

Here, a stochastic context-free grammar (SCFG) can be obtained by specifying a probability for each production in a contextfree grammar.

SCFGs extend context-free grammars in the same way that HMMs extend regular grammars.

It is a more expressively powerful class of stochastic grammars than the HMMs.

A pioneering work was performed by Sakakibara et al.

(1994), extending the notion of profile HMMs (Eddy, 1998; Krogh et al., 1994) to profile SCFG.

They assessed the ability of trained SCFGs to perform three tasks: to discriminate transfer RNA (tRNA) sequences from nontRNA sequences, to produce multiple alignments, and to ascertain a secondary structure of new sequences.

Knudsen and Hein (1999) also suggest SCFGs as an alternative probabilistic methodology for modeling RNA structure (Knudsen and Hein, 1999).

For representative databases based on SCFG, RFAM (Gardner et al., 2009; Griffiths-Jones et al., 2003) of modeling common non-coding RNA families by stochastic context-free grammars called covariance models, can be cited (Eddy and Durbin, 1994).

SCFGs have also been used for alignments of sequences.

Pair stochastic context-free grammars (PSCFGs) have been studied for alignments of a pair of RNA sequences without any prior information about their secondary structures (Holmes and Rubin, 2002; Rivas and Eddy, 2001).

PSCFGs are a generalization of stochastic context-free grammars and can generate an aligned pair of sequences.

Later, the notion of pair HMMs defined on alignments of linear sequences is extended to pair stochastic tree automata, called Pair HMMs on Tree Structures (PHMMTSs) (Sakakibara, 2003), defined on alignments of trees.

PSCFGs are used by the multiple structural alignment softwares such as Stemloc (Holmes, 2005).

In (Chuong et al., 2006), the authors used a secondary structure prediction method based on conditional loglinear models (CLLMs), a flexible class of probabilistic models that generalize upon SCFGs by using discriminative training and featurerich scoring.

Dowell and Eddy (2004) studied the tradeoffs between model complexity and prediction accuracy, by comparing nine different small SCFGs, and concluded that SCFG designs have prediction accuracies near the performance of free energy minimization models (Dowell and Eddy, 2004); still, probabilistic methods have not replaced free energy minimization methods as the tool of choice for secondary structure prediction, as the accuracies of the best SCFGs have yet to match those of the best physicsbased models.

Recently, Dyrka and Nebel (2009) a framework, based on the combination of stochastic contextfree grammars related to different physicochemical properties of amino acids and on genetic algorithms, which was shown to produce relevant protein binding site descriptors (Dyrka and Nebel, 2009).

The presence of pseudoknot secondary structures of noncoding RNA molecules and the presence of repeats of many varieties in biological sequences indicate the need to use more powerful grammar formalism.

Modeling various repeat sequences, or the pseudoknot structures of RNAs is beyond the generative power of contextfree grammars and inevitably involves the complexity of contextsensitivity.

Type1 grammars (contextsensitive grammars), in Fig.

2, generate the contextsensitive languages.

Context sensitive grammar is a formal grammar (N, T, P, S) such that all of the production rules are of the following forms:  αAβ → αγβ  where A ∈ N , α,β ∈ (N U T)* and γ ∈ (N U T)+ are applied.

Actually, the term, “contextsensitive” comes from the fact that a symbol can have different interpretations, depending on where it appears in the input language.

Thus, unlike contextfree grammars, more than one symbol can appear on the lefthand side of the grammars.

For example, we can specify a grammar representing {antngn | n> 0} in the following way:    A → aATG   A → aTG   aT → at   tT → tt   GT → TG   G → g  Using the rules above, the string “aattgg” can be derived from the non-terminal A, by applying a series of derivations: A → aATG → aaTGTG → aatGTG → aatTGG → aattGG → aattgG → aattgg.

A few attempts have been made to represent pseudoknots.

Rivas and Eddy (Rivas and Eddy, 1999) suggested a formal transformational grammar that avoids the use of general context-sensitive rules by introducing a small number of auxiliary symbols used to reorder the strings generated by otherwise context-free grammar.

Sometimes, special grammar formalisms, classified as mildly context-sensitive grammars, are used (Joshi et al., 1988).

Uemura et al.

(1999) defined two subclasses of tree-adjoining grammar (Joshi et al., 1975) called sl-tag and esl-tag, and argued that esltag is appropriate for representing RNA secondary structures including pseudoknots (Uemura, 1999).

Matsui et al.

(2005) proposed the pair stochastic tree-adjoining grammars (PSTAGs) for modeling pseudoknots, showing that their method significantly improves the prediction accuracies of RNA secondary structures (Matsui et al, 2005).

Abe and Mamitsuka (Abe and Mamitsuka, 1999) studied a more powerful class of grammar, called stochastic ranked node rewriting grammars, than SCFGs and applied them to the problem of secondary structure prediction of proteins, concentrating on the problem of predicting β-sheet regions, to capture the parallel and anti-parallel dependencies and their combinations.

Rivas and Eddy (2000) introduced a new class of grammars for deriving RNA secondary structure by a sequence with a single hole (Rivas and Eddy, 2000).

The grammar is based on a number of auxiliary symbols used to reorder the strings.

Cai, Malmberg, and Wu (2003) described a formal transformational grammar that extends context-free grammar, based on parallel communicating grammar systems (Cai et al., 2003).

The key feature is the use of special non-terminal symbols that dictate specific rearrangements of substrings in a derivation.

Context-sensitive grammar formalisms have also been used to model non-coding RNAs.

To model ncRNA precursors, Yoon and Vaidyanathan (2004) proposed a context-sensitive HMM (CSHMM), which is an extension of the idea of HMMs by introducing a memory, in the form of a stack or a queue (Yoon and Vaidyanathen, 2004).

Later, Agarwal et al.

(2011) extended the idea slightly and proposed a CSHMM structure with two contextsensitive states to model miRNA sequences (Agarwal et al., 2011).

Patridge et al.

(2009) used similarities between DNA and natural languages (Baquero, 2004) to develop a contextsensitive grammar to define cassette arrays.

Also, Tsafnat et al.

(2009) presented a method to discover higherorder DNA structures, using a contextsensitive deterministic grammar (Tsafnat et al., 2009).

These grammars have been applied to the discovery of gene cassettes associated with integrons.

On the other hand, there has been an attempt to model RNA structures with pseudoknots, using just contextfree grammars by adding four building blocks of genus to the conventional secondary structures (Reidys et al., 2011).

Reidys et al.

(2011) used the natural topological classification of RNA structures, resulting in corresponding unambiguous multiple contextfree grammars to provide an efficient dynamic programming approach.

In this review, we tried to gather a list of published works, categorized by the expressive power of formal grammars.

The lists might not be exhaustive, and there is a possibility that some of the works, presented here, could have been misclassified, because not all the works fall within the traditions of the Chomsky hierarchy.1)  The genome may not be just a molecule with patterns.

It may be a language, and an information storage mechanism.

Precisely constructed sequence models for linguistic structure can play an important role in the process of biological discovery itself.

In this respect, grammatical representations have increasing importance in the field of bioinformatics for biological sequence analyses.

It is noteworthy that a recent comparison of the predictive power of learned grammars against an expertdeveloped grammar shows the possibility that an inferred grammar can represent a general model that accurately identifies structures without referring to prior knowledge about them (Tsafnat et al., 2011).

More positively, a linguistically modeled biological sequences may automatically provide solutions for thorny biological problems and thus provide us a deeper understanding of genome.

