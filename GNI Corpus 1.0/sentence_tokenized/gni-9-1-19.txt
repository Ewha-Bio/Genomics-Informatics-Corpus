Standard-based Integration of Heterogeneous Large-scale DNA Microarray Data for Improving Reusability.

Gene Expression Omnibus (GEO) has kept the largest amount of gene-expression microarray data that have grown exponentially.

Microarray data in GEO have been generated in many different formats and often lack standardized annotation and documentation.

It is hard to know if preprocessing has been applied to a dataset or not and in what way.

Standard-based integration of heterogeneous data formats and metadata is necessary for comprehensive data query, analysis and mining.

We attempted to integrate the heterogeneous microarray data in GEO based on Minimum Information About a Microarray Experiment (MIAME) standard.

We unified the data fields of GEO Data table and mapped the attributes of GEO metadata into MIAME elements.

We also discriminated non-preprocessed raw datasets from others and processed ones by using a twostep classification method.

Most of the procedures were developed as semi-automated algorithms with some degree of text mining techniques.

We localized 2,967 Platforms, 4,867 Series and 103,590 Samples with covering 279 organisms, integrated them into a standardbased relational schema and developed a comprehensive query interface to extract.

Our tool, GEOQuest is available at http://www.

snubi.

org/software/GEOQuest/Keywords: gene expression data, data integration, classificationAfter genome sequencing, DNA microarray analysis has become the most widely used source of genomescale data in the life sciences (Allison et al., 2006; Brazma et al., 2001).

DNA microarray is a highthroughput and dataintensive technology that provides the means of measuring the expression of thousands of genes or proteins simultaneously and brings the unprecedented development in the informatics and analysis aspect (Chaussabel and Sher, 2002; Quackenbush, 2002).

Since this technique provides researchers with comprehensive understanding of biological complex features, it has been used in not only biological but also clinical field.

As many microarray experiments generate large data sets that can contain tens to hundreds of samples, however, it has needed to be managed systematically with computational tools due to their own complexity.

Moreover, requirements for ensuring the scientific integrity of data and sharing the data have given rise to development of public microarray repositories like GEO (Barrett et al., 2007), ArrayExpress (Parkinson et al., 2007), Stanford Microarray Database (SMD) (Gollub et al., 2003), keeping pace with the standardization (Edgar and Barrett, 2006; Perou, 2001).

With appearance of them, the data generated in one laboratory can be available to other researchers and various analytical methods uncover different biological insights.

Especially, NCBI GEO has kept the largest amount of gene expression data which have grown exponentially, currently holdingover 200,000 samples.

Recently, researches to find clinical or biological meaning using GEO data have been actively performed to show its reusability.

Butte et al.

(2006) constructed PhenomeGenome network, Disease Nosology and GeneBehavior Disease through analysis of huge amounts of gene expression data in GEO using Unified Medical Language System (UMLS) (Humphreys et al., 1998).

Yoon et al.

(2006) constructed an application tool to provide the present largescale approach for the analysis of GEO microarray data.

Though GEO has abundant practical possibilities, it does not support the standard format or model but its selfstructured format.

Moreover, it stores the microarray data without distinguishing processed data and raw data.

These factors hinder providing a comprehensive biological analysis environment and future integration of   the external resources and make extracting the desired information difficult.

Currently, some tools have been developed to utilize large-scale GEO data - SeqExpress (Boyle, 2005), GEOQuery (Sean and Meltzer, 2007), ArrayQuest (Argraves et al., 2005).

However, there is a limitation that they can handle only GEO DataSets which are curated by GEO staffs.

To solve the problem and overcome the limitation, we attempted integration of heterogeneous microarray data in GEO using microarray data standard, Minimum Information About a Microarray Experiment (MIAME).

In GEO, it encourages submitters to supply MIAME standard compliant data.

However, it is not related to the format, but rather to the content provided.

We performed data field unification of GEO Data table and distinguished between raw data and transformed data using classification method.

To integrate the data in an efficient and accurate manner, these processes were developed in both manual and semiautomated way (Vita et al., 2006).

Biological research increasingly depends on computational analysis of data (Miotto et al., 2005).

A prerequisite for computational analysis is the availability of experimental data in a formalized, structured and machine-readable format.

GEO offers DataSet (GDS) which represents a curated collection of biologically and statistically comparable data for computational analysis method.

However, there is a limitation that GEO data are reassembled mainly for not entire GEO data but a unit of GEO Series (GSE) only.

To overcome the limitation and provide a comprehensive biological analysis environment, we attempted integration of heterogeneous microarray data in GEO using microarray data standard, Minimum Information About a Microarray Experiment (MIAME).

Workflows are as follows (Fig.

1): GEO data localization, Data field unification, Data transformation classification, Standard based integration of heterogeneous microarray data, and GEO data update.

GEO is an international repository for gene expression data.

It is developed and maintained by the National Library of Medicine (NLM).

It serves as a public repository for a wide range of highthroughput experimental data like single and dual channel microarraybased experiments measuring mRNA, miRNA, genomic DNA and protein abundance.

In GEO, the basic entity types are Platform, Sample and Series.

Platform includes a summary description of the array (Descriptive information) and a data table defining the array template (Data table).

Each row in the data table corresponds to a single element, and includes sequence annotation and tracking information as   provided by the submitter.

Sample includes a description of the biological source and the experimental protocols to which it was subjected (Descriptive information), and a data table containing hybridization measurements for each element on the corresponding Platform (Data table).

Series defines a set of related Samples considered to be part of a study, and describes the overall study aim and design.

Each of these entities is assigned an accession number that may be used to cite and retrieve the records.

Each entity type has ‘Descriptive information’ for describing each entity’s correspondent information.

Platform and Sample have ‘Data table’ which consists of ‘Header’ and ‘Matrix’.

Header identifies the attributes of each column of Matrix.

Matrix includes each entity’s correspondent data contents.

We accessed and downloaded all GEO data on January 22, 2007.

Python language and MySQL DataBase Management System were used in Linux system for localizing GEO database.

Since data in GEO’s File Transfer Protocol were updated late at that time, we used HyperText Markup Language (HTML) documents from which users can see web page.

Accession Display  accession to the data through GEO accession number  was used to automatically download all GEO data.

Using specific HTML tag structure, we localized all GEO data.

To integrate increasingly large volume of GEO data, formalizing loosely-defined format of GEO is an indispensable process.

First, we determined representative fields and constructed mapping tables for Data table of three technology types - ‘spotted DNA/cDNA’, ‘spotted oligonucleotide’, and ‘in situ oligonucleotide’.

The criteria of determining representative fields for data field unification are shown in Table 1.

To construct each mapping table, we adapted a simple text-mining method and performed through manual curation with checking a description and values of each field.

Through the mapping tables, we mapped fields of all GEO Data table into each representative field.

Five database tables are used to store the GEO Data table for Platform, dual channel Sample, and three technology types in single channel Sample.

Ambiguous cases on mapping them are treated by EAV (EntityAttribute Value) table (Johnson et al., 1997) to prevent loss of data caused by the mapping process.

After inserting data into each result table is completed, the four tables of Sample are integrated through the next stepThe GEO database architecture is designed for the efficient capture and storage of heterogeneous highthroughput data sets.

The structure is sufficiently flexible to accommodate evolving state of the art technologies (Barrett and Edgar, 2006).

Consequently, the data have many different styles and comprise varying contents.

Due to the flexibility, GEO stores the data which include the words having the same concept but different spelling or some misspelled words without any control process.

This characteristic increases heterogeneity between data in GEO and makes future integration of this resource with other biological and clinical data difficult.

Data standard is an essential requirement for representation of information to ensure proper semantic integration of heterogeneous data, and also for communication standards to ensure interoperability between disparate data sources (Louie et al., 2007; MartinSanchez et al., 2004).

In microarray data, the introduction of the MIAME standard has been a great success (Rayner et al., 2006).

However, the data format of GEO does not follow the standard.

Therefore, it hinders semantic integration and interoperability between heterogeneous microarray data.

For the reason, we customized the GEO data into MIAME standardbased format.

First, we analyzed attributes of Descriptive information in each GEO entity to understand that which GEO attribute corresponds to which part of MIAME.

Second, we mapped GEO attributes to elements in each part of MIAME and stored them into the database.

Third, we mapped values of some GEO attributes into controlled terms from the MGED Ontology (http://mged.

sourceforge.

net/ontologies/index.

php).

For example, we performed mapping from ‘technology type’ in Platform to TechnologyType class and from ‘type’ in Series to both MethodologicalDesign class and ExperimentalFactor class.

In case of the ‘type’ in Series, there are the multiple values in one text, we split the text into single values.

In the second and third processes, we performed both a simple text mining method and manual curation with checking a description and values of each field.

Finally, microarray data in which data field unification process is done are stored into one database table according to classification result between loglike transformed data and raw data.

The hypothesis underlying microarray analysis is that the measured intensities for each arrayed gene represent its relative expression level.

Before the levels can be compared appropriately, a number of transformations must be carried out on the data to adjust the measured intensities to facilitate comparisons and to select genes that are significantly differentially expressed (Quackenbush, 2002).

In microarray data analysis, difference between raw data and transformed data affects analysis results importantly.

Moreover, MIAME provides the conceptual structure for the representation of microarray data including raw and processed data (Brazma et al., 2001; Rayner et al., 2006).

However, both raw data and transformed data are stored together in GEO with no separation.

For the reasons, we must distinguish between transformed data and raw data.

Even though submitters are encouraged to describe about a process of Sample ‘Value’ field which indicates final expression value measurements, a large part of ‘Value’ fields have not only an ambiguous description or no description but even wrong description (Table 2).

In this paper, we assumed that the log transformation are mainly used among the several transformation processes and attempted to distinguish raw data and loglike transformed data.

We treated some transformation method as loglike transformation (Table 3).

We propose a model for data processing classification using machine learning techniques.

Selection of a training data set For most classification study, a training data set consisting of records whose class labels are known must be provided.

It is necessary to select training data properly because the training data set is used to build a classification model.

First, we performed simple random sampling and extracted 200 GEO Samples.

Sampling is a commonly used approach for selecting a subset of the data objects to be analyzed.

Next, we classify data manually in compliance with three criteria for giving the correct class to data.

• Description of ‘Value’ field in GEO Sample  • Range of data distribution  • Symmetricity of distribution  In the classification process, guarantee of data quality is very important factor.

We trimmed twotailed 5% values of the distribution in each Sample to remove outliers before selection of training data.

We extracted 188 GEO Samples for three criteria and emailed to data submitters of 12 Samples, the rest of the data set.

As a result, 190 Samples are determined as training set (96 Loglike transformed data, and 94 Notlog transformed data).

Learning process for classification modelWe determined the features that can explain a difference between two classes to create a classification model.

The first one is the difference of the skewness values between original distribution and its loglike transformed distribution (DSD).

Skewness is a measure of the asym  metry of the data of the probability distribution.

where χi is each value of the distribution and x represents the mean value of the distribution.

n is the number of values in the distribution.

If expression values of raw data are loglike transformed, the skewness of original distribution is changed remarkably (Fig.

2).

However, if those of loglike transformed data are log-like transformed again, the skewness is changed slightly.

This characteristic makes the feature available for the classification model.

Second, a maximum value of data (MD) is concerned.

Common image scanners generate typically 16bit Tagged Information File Format (TIFF) images.

Therefore, the log-like transformed data have rarely over 16.

On the other hand, raw data can have values more than thousands.

Since the maximum value can be a good factor for the classification process, we include this feature.

To determine the classification model, we adopted the logistic regression method with these features.

where y is defined as above and xk,i is the value of features of ith GEO Sample data.

k is the number of features and n is the number of GEO Samples.

The logistic regression has several strengths.

1) It is not restricted to data.

2) It can represent the model easily.

3) It can classify the data into each group with ease.

4) It finds the best explanatory variable.

We can predict the class, log-like transformed or not for any GEO Sample data using the output computed by the model.

If y is the positive value, the data is classified as loglike transformed data.

If y is the negative value, it is classified as not log-like transformed data.

Set-wise validation of classification result10 Sample data excluded from 200 Sample data were used to validate the classification model.

We performed classification of 10 data and case-study of its result.

A GEO Series is a group of related GEO Samples.

Thus, we can assume that Samples in a Series should be classified as same class.

However, it is found that Samples are classified with different class in a Series.

To solve this case, we adopted simple voting method and classified Samples in a Series as a class with which Samples are classified more.

As an amount of GEO data grows exponentially, we need to update the data continuously.

For incremental update, we extract a list of GEO accession numbers and release date in our database.

Next, if a new data is not in the list, we download, process, and store it into the database.

To trace the update status, accession numbers of updated data and the updated date are written in a log file.

Besides, authority of data in GEO changed often public to private, and vice versa.

We also recorded the accession numbers of the data in the log file.

Update workflow is shown in Fig.

3.

In this section, we focused on constructing the mapping tables for each technology type and each manufacturer.

Through a simple textmining method and manual curation, we mapped the fields which are written in various strings into one representative field.

Table 1 shows the example of words which have various strings for a concept.

There are around 3,000 distinct fields in Platform and Sample Data tables respectively.

Since most of the data fields are recorded irregularly, we have to unify them manually.

On the other hand, the fields of dual channel   in Sample are stored in partially regular pattern.

We performed a simple text mining method (exact match) and obtained a precision of 0.8474 (544/642), a recall of 0.2011 (544/2,705), and the F-measure of 0.3248.

Through its method was not reliable as the result shows, it was helpful to reduce a timeconsuming process.

The unification result is shown in Table 4.

We designed a core relational database according to the MIAME standard-based format.

In it, the GEO data is customized according to the result of mapping GEO attributes to elements in each part of MIAME.

A comprehensive view of mapping result is presented in Table 5.

We mapped values of attributes in GEO (‘technology type’ in Platform, ‘type’ in Series, ‘Extracted molecule’ and ‘Label’ in Sample) into terms from MGED Ontology (TechnologyType, MethodologicalDesign and ExperimentalFactor, and LabeledExtract class) respectively.

Like data field unification process, we performed a simple text mining method.

Among 4,495 values in GEO, 2,537 values are mapped to MGED Ontology and 1,958 values are stored as they are.

Other database tables have been implemented in order to collect data regarding all submitters, laboratories or organizations which take part in each experiment and to handle the case that a Sample included in multiple Series.

All data regarding microarray experiment results (Data table) are stored in the additional database tables according to its transformation characteristic.

Finally, we hold 2,967 Platforms, 103,590 Samples (57,052 single channels and 46,538 dual channels) and 4,867 Series in our database.

In the database, we investigated the distribution of organism, source, and extracted molecule in microarray experiments (Table 6).

The view of the results reflects that the most interesting experiments in the present researches have been concerned with Homo sapiens.

Moreover, it shows that the breast tissue is mostly used as a material for experiments and that breast cancer is a matter of primary concern in cancer research.

In a part of Extracted Molecule, the distribution means the data stored in GEO mainly result from arraybased experiments in which researches of transcriptional pattern is accomplished.

Classification using Logistic Regression modelThe initial classification is done by logistic regression, which is used when users have a binary dependent variable.

The training data set currently consists of 190   examples.

After learning process, we can obtain the classification model and the contingency table as a learning result (Table 7).

As we can see the result, we obtained a sensitivity of 1.0 (96/96) and a specificity of 1.0 (94/94).

Set-wise validationTo test the classification model, we performed classification for the 10 Sample data excluded from 200 Sample data which were sampled randomly to make training data set.

We assumed that basically, Samples in a Series have same classification results.

Among the 10 classification results, however, we found a questionable result.

Though two Samples are included in same Series, one was classified as Log-transformed data and the other was not.

For the reason, we considered Series as a set and attempted the setwise validation.

We applied our classification model to entire data set and extracted the data which correspond to questionable case.

Entire data consist of 3,911 Series which include 97,026 Samples.

The reason why the number of entire Samples is not 103.590 is that GEO Values of 6,564 Samples have only ‘null’ string or zero value.

As a result, we found 103 Series which include 4,876 Samples.

These data were classified by humans to validate the classification model (Table 8).

To solve this problem, we applied a simple voting method.

For example, if the number of loglike transformed class is more than that of notlog transformed one in a Series, all Samples in the Series are assigned to the loglike transformed class.

If the number of loglike transformed class is equal to that of the notlog transformed one, all Samples are assigned to what they are classified as.

We tested both the original classification model and a combination of the model and voting method on data sets differently classified in a Series (DCS) and entire data set, respectively.

The results are presented in Table 9 with various evaluation measurements.

As a result of classification for all GEO data, 56,012 loglike transformed data and 41,014 notlog transformed data are stored into separated database tables.

The integrated database can be queried on the World Wide Web at http://geo.

snubi.

org/~geoxperanto/html/   GEORetrieval/GEORet_Inter.

html (Fig.

4) This interface enables users to search hybridization-centered data.

In comparison with free text search of GEO Entrez Search System, users can search the database efficiently due to partially itemized GEO terms.

Some main issues of management in production of microarray experiments are the large amount of information produced and their heterogeneity.

Therefore, it is important to make independently collected microarray data conform to standard for sharing of data efficiently and be comparable with each other.

The MIAME standard can serve both bioinformaticians and biologists to deal with the former issue.

To solve the latter issue, microarray data must be classified by transformation method.

In the process of data classification, we found that a combination of a classification model and other method can boost the performance of classification (Chen et al., 2006).

We have presented a simple but effective two-step method.

It consists of a Logistic Regression model as well as a simple voting method.

Recently, it is clear that these datasets should be combined to generate a more comprehensive understanding of underlying biology.

With appropriate integration of heterogeneous microarray data in GEO into the standard-based database, improvement of analysis results and comparison of data from different experiments can be possible.

Integration strategies we proposed allow the GEO to progress remarkably toward a more standardized repository and to serve as a more uniform platform for microarray data analysis.

Also, published research studies using GEO data can be expanded and improved with our database and analysis approaches (Yoon et al., 2006) which have been published.

Yet, we have some problem to solve further.

In data field unification, we can not handle polysemy problem, which means the case that a word has many meanings.

An important next step is to adapt natural language processing method to solve not only the problem but also new terms which will be input in GEO.

In setwise validation of data transformation classification, we assumed that all Samples in a Series have same classification results.

This assumption may miss some case of a false positive result in a Series for entire data set.

With human validation for entire data set, we can correct our validation result.

At the conclusion, we suggest to data submitters that they submit their data with the correct description in the unified format to collaborate with researchers in other fields or to provide machinereadable data for computational postanalysis.

The effort may lead to improve the computational analysis results for the discovery of remarkable biological knowledge.

This study was supported by a grant from a Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (20100028631).

YRP’s educational training was supported by the Ministry of Health & Welfare, Republic of Korea (A040002).

