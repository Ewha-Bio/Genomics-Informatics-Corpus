Tokenization, or word segmentation, is an indispensable first step in natural language processing (NLP). Unlike languages such as English, in which sentences are already segmented into words by spaces, tokenization is not a trivial process in Japanese, where usually no word delimiters are provided.
Systems called morphological analyzers, which tokenize sentences and assign parts of speech to the tokens, are used for this purpose, and most current morphological analyzers require dictionaries. In this process, biomedical text requires special dictionaries, because technical terms often cause problems as out-of-vocabulary terms.
In BLAH5 (Biomedical Linked Annotation Hackathon 5), the current status of Japanese medical vocabularies, especially the Japanese version of Medical Subject Headings (MeSH) is reviewed, and the creation of an open-source alternative is attempted.