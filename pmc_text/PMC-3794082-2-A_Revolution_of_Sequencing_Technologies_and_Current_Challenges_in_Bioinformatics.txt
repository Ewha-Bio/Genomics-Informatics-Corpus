The recent development of sequencing technologies has enabled us to assess much deeper layers of microbial communities by generating tons of nucleotide sequences at lower costs. NGS technologies have revolutionized the field of microbial ecology, as they have allowed researchers to reach the true level of diversity more closely through more in-depth sequencing. There are various applications using these NGS platforms, ranging from single-gene targeted sequencing to whole-genome sequencing and shotgun metagenome sequencing. Details about the principles and applications of each NGS technique have already been reviewed elsewhere [5]. The first NGS platform, pyrosequencer (GS20), generated just 20 Mb per run with a read length of 100 bp on average. A recent version of pyrosequencer (GS-FLX Titanium) has an advantage over other NGS techniques, especially 16S rRNA gene-based surveys, as it produces around ~500 Mb per run with a longer read length (400-500 bp), which is sufficient to cover partial hypervariable regions of 16S rRNA. Illumina sequencing produces many more reads with cheaper price that are more accurate (accuracy of >99%) than 454 platforms (accuracy of 98.93%) [6] but is somewhat limited in certain fields due to the relatively shorter read length (<100 bp) of early versions. However, Illumina became more popular, since it gradually improved readable lengths (i.e., 2 Ã— 250 bp in MiSeq). More recently, Pacific Biosciences has released a new sequencing technology, PacBio RS, and Oxford Nanopore Technologies introduced GridION/MinION devices, both of which allow single-molecule sequencing with a much longer read length. However, further improvements are necessary for use in practice due to the high intrinsic errors (10% to 15% in PacBio and around 4% in GridION) [7].
Since the first launch of the NGS platform in 2006, novel sequencing technologies have been developed continuously and rapidly. As a result, massive sequence data produced by NGS technologies are accumulating at an unflagging rate. However, the computing power and development of algorithms needed to deal with the huge datasets efficiently are not keeping up with the speed of data production. For example, access to sequence data is still hampered by unsuitable data storage systems, such as short read archive (SRA), and many early papers were not accompanied by data deposition into public databases. Repositories should be big enough to be ready to allow the increasing volume of upcoming sequence data, and all data must be deposited in a standardized manner. In addition to the shortage of data storage space and confounding submission formats, the characteristics of the produced sequence data pose another problem in further processing. For instance, shorter read length, which is an inevitable limit of high-throughput sequencing, is a barrier for sequence assembly, and the relatively higher rates of sequencing errors compared with previous Sanger methods also make it hard to recover genuine sequences.