Let Ti denote the survival time for the ith individual and xi and zi denote the predictor variable vector coding a gene-gene and gene-environment interaction of interest and the vector coding for the covariates, respectively. Let β and γ be the corresponding parameter vectors to xi and zi, respectively. Then, we call β the target effects and γ the covariate effects. The parametric regression model represents the linear relationship between the log survival time and covariates as follows: Y = logT = µ + β'X + γ'Z + σW.
Here, µ is the mean value of the log survival time when X = Z = 0, σ is a scale parameter, and W is the error distribution. When T has a Weibull distribution, W has a standard extreme value distribution. For a log-logistic distribution, W has a standard logistic distribution.
The standardized residual for the ith individual, si=logTiμ^Y^′Ziσ^, is obtained from the null model of no target effects (i.e., β = 0). Since the standardized residual is the difference between the observed and expected survival time under the null model with no SNP effects, the magnitude and sign of the standardized residual provide a measure of the association between SNPs and survival time. Those patients who have positive standardized residuals are at low risk for the disease, because one survives longer than expected under the null model. Similarly, those patients who have negative standardized residuals are at high risk for the disease, because one dies earlier than expected. Thus, each individual with a positive standardized residual is classified as a control, whereas one with a negative standardized residual is classified as a case. In addition, for each multi-locus genotype combination of SNPs, we calculate the sum of the standardized residuals of those patients who have the corresponding genotype and replace the case-control ratios with the sum of the standardized residual to discriminate between high- and low-risk groups. We assign the cell as low-risk if the sum of standardized residuals within that cell is greater than or equal to 0 and as high-risk otherwise. The process of AFT-MDR is done by following the algorithm of the original MDR through 10-fold cross-validation to select the best pair of SNPs.
We propose to improve the process of AFT-MDR by combining the unified model-based MDR to test the significance of gene-gene interactions. In the first step of the proposed method, multi-level genotypes are classified into high-risk and low-risk groups, as done in AFT-MDR. Then, we define an indictor variable, S, as 1 for the high-risk group and 0 otherwise. In the second step, the variable S is considered with the other adjusting covariates in the accelerated failure time regression model. The testing for the significance of S implies that there is a significant gene-gene interaction associated with survival time. For testing the significance of S, a Wald-type test statistic is used, and its asymptotic distribution is a chi-square distribution under the null hypothesis of no gene-gene interaction. However, as described in Yu et al. [9], the asymptotic distribution of the Wald-type test statistic is not a central chi-square distribution, because the expectation of the test statistic is not 0 under the null hypothesis. To adjust for the bias of the test statistic, non-centrality is estimated by a small number of permutations—say, 5 or 10 times. Based on the non-central chi-square test statistic, the significance of a gene-gene interaction can be tested for all possible pairs of SNPs without any intensive permutations. The proposed method easily tests the significance of a gene-gene interaction for all possible higher-order pairs of SNPs in the framework of a regression model. It allows for the adjustment of covariates and the main effect of SNPs, while the original MDR method cannot.