Optimality of IVW
IVW maximizes likelihood function
We will define that a method is optimal if the method achieves a specific goal more effectively than any other method. We show that IVW is optimal in two different aspects: (1) the summary estimator gives the greatest likelihood than any other estimator and (2) the summary estimator's variance is smaller than the variance of any other estimator. First, we show that IVW is optimal in the sense that the IVW estimator maximizes the likelihood function. Suppose that we have a series of n studies with observed effect sizes Xi, i = 1, 2, … n. Under the fixed effects assumption, there exists a true effect size µ, and each observation Xi comes from a normal distribution with mean µ and a standard deviation σi. The probability density function of each observation is given by fXi|µ, σi2=12πσi2e−Xi−µ22σi2. (8)
Because −lnL(µ,σi2|X1,…,Xn) will be minimized at d(−lnLµ, σi2X1, …, Xn)dµ=0 , we can obtain the maximum likelihood (ML) estimator:  µ^=∑i=1nσi2−1Xi∑i=1nσi2−1, (9) which is equivalent to the IVW statistic X¯ in equation (2). Therefore, the inverse variance-weighted average method is optimal in the sense that it maximizes the likelihood of observations with the optimized weight of Wi = (σi2)−1 [2122].
IVW achieves minimum variance
IVW is optimal in the sense that the IVW estimator achieves minimum variance. In short, IVW achieves minimum variance by the properties of maximum likelihood estimator (MLE). MLE has the following property, as shown by Greene [27]: if the sampling is from an exponential family of distributions and the minimum variance unbiased estimator (MVUE) exists, that estimator becomes the ML estimator [27]. Thus, by this property and under these conditions, we can conclude that IVW achieves minimum variance, because IVW is the MLE and MVUE [27].