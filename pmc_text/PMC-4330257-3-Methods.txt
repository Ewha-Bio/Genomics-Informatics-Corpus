Mutual information
We used mutual information measures to assess the strength of the association between a pair of SNPs and the disease status of gastritis. Mutual information has been widely used to measure dependence or independence between two random variables [11, 12, 18, 19]. It is a non-parametric measure and is able to detect both linear and non-linear associations [14]. This measure is based on Shannon's entropy, H(X) = Σx∈X - p(x)log(p(x)), which shows the uncertainty of the random variable X. Mutual information I(X;Y) between random variables X and Y is defined by the composition of entropy as follows: I(X;Y) = H(X) + H(Y) - H(X,Y).
H(X), H(Y) denote entropies for the random variables X, Y, and H(X,Y) denotes the joint entropy of the two random variables as follows: H(X,Y)=Σx∈XΣy∈Y - p(x,y)log(p(x,y)).
A high mutual information value indicates a strong association between two random variables. The measure can also be extended to assess the strength of association between a pair of SNPs and a phenotype. The extended version of mutual information is as follows: I(X1,X2;Y) = H(X1,X2) + H(Y) - H(X1,X2,Y), where X1 and X2 are random variables for two SNPs, and Y denotes random variables for the disease. In our recent work [12], we have shown that this measure could identify high-order epistatic interactions both with and without marginal effects by using simulation models, such as in Culverhouse et al.'s [20] and Velez et al.'s studies [21].
As the genotype and disease status are represented as discrete values, it is more convenient to consider the random variables as a partition of the combination of the genotypes and disease status. Then, the entropy of a random variable X can be represented in terms of the partition as follows:
where X = {A1, A2,…, An} is a partition on the set of samples S = {A1∪A2∪…∪An}, and no intersections exist between elements in the partition. The joint entropy of two random variables for the partition of S, X = {A1, A2,…, An} and Y = {B1, B2,…, Bm} is defined as follows:
The entropy also can be extended to the joint entropy of multiple random variables (e.g., 3 or 4 SNPs) naturally.